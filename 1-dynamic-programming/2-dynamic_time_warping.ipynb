{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc048d56-62ad-4b17-a523-b6201f9687f4",
   "metadata": {},
   "source": [
    "# Assignment 1: Dynamic Time Warping\n",
    "\n",
    "---\n",
    "\n",
    "## Task 4) Isolated Word Recognition\n",
    "\n",
    "Due to the relatively large sample number (e.g. 8kHz), performing [DTW](https://en.wikipedia.org/wiki/Dynamic_time_warping) on the raw audio signal is not advised (feel free to try!).\n",
    "A better solution is to compute a set of features; here we will emtract [mel-frequency cepstral coefficients](https://en.wikipedia.org/wiki/Mel-frequency_cepstrum) over windows of 25ms length, shifted by 10ms.\n",
    "Recommended implementation is [librosa](https://librosa.org/doc/main/generated/librosa.feature.mfcc.html).\n",
    "\n",
    "### Data\n",
    "\n",
    "Download Zohar Jackson's [free spoken digit dataset](https://github.com/Jakobovski/free-spoken-digit-dataset).\n",
    "There's no need to clone, feel free to use a revision, like [v1.0.10](https://github.com/Jakobovski/free-spoken-digit-dataset/archive/refs/tags/v1.0.10.tar.gz).\n",
    "File naming convention is trivial (`{digitLabel}_{speakerName}_{indem}.wav`); let's restrict to two speakers, eg. `jackson` and `george`.\n",
    "\n",
    "### Dynamic Time Warping\n",
    "\n",
    "[DTW](https://en.wikipedia.org/wiki/Dynamic_time_warping) is closely related to [Levenshtein distance](https://en.wikipedia.org/wiki/Levenshtein_distance) and [Needleman-Wunsch algorithm](https://en.wikipedia.org/wiki/Needlemanâ€“Wunsch_algorithm).\n",
    "The main rationale behind DTW is that the two sequences are can be aligned but their speed and exact realization may very.\n",
    "In consequence, cost is not dependent on an edit operation but on a difference in observations.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638e594f-2ef3-4d92-862c-4bb6a01ff21b",
   "metadata": {},
   "source": [
    "### Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9b8ab0-b4d8-483f-8eab-a0112bab5e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa as lr\n",
    "import os\n",
    "from typing import List, Tuple, TypedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdd0b2f-5e6c-4fb4-95b0-217e75aa5b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: Read in files, compute MFCC, and organize\n",
    "### Notice: You can restrict the number to a few files for each speaker-digit\n",
    "\n",
    "class Audio(TypedDict):\n",
    "    digitLabel: int\n",
    "    speakerName: str\n",
    "    index: int\n",
    "    mfccs: List[Tuple[float]]\n",
    "\n",
    "audios: List[Audio] = []\n",
    "\n",
    "speakers = [\"george\", \"jackson\", \"yweweler\"]\n",
    "\n",
    "### YOUR CODE HERE\n",
    "FOLDER = 'data/recordings'\n",
    "\n",
    "for file in os.listdir(FOLDER):\n",
    "    if not file.endswith('.wav'):\n",
    "        continue\n",
    "\n",
    "    parts = file.split('_')\n",
    "    digit, name, index = int(parts[0]), parts[1], int(parts[2].split('.')[0])\n",
    "\n",
    "    if name not in speakers or index > 9:\n",
    "        continue\n",
    "\n",
    "    y, sr = lr.load(FOLDER+os.sep+file)\n",
    "\n",
    "    audios.append(Audio(\n",
    "        digitLabel = digit,\n",
    "        speakerName = name,\n",
    "        index = index,\n",
    "        mfccs = lr.feature.mfcc(y=y, sr=sr, n_mfcc=10, hop_length=int(0.010 * sr), win_length=int(0.025 * sr))\n",
    "    ))\n",
    "### END YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125e920b-3296-44aa-87b9-81d73f515cc3",
   "metadata": {},
   "source": [
    "### Implement Dynamic Time Warping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7476e6-be89-41c5-81f4-dd6ab331a748",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist(x: Tuple[float], y: Tuple[float]) -> float:\n",
    "    \"\"\"\n",
    "    Compute the distance between two samples.\n",
    "\n",
    "    Arguments:\n",
    "    x: MFCCs of first sample.\n",
    "    y: MFCCs of second sample.\n",
    "\n",
    "    Returns the distance as float\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    max_len = max(len(x), len(y))\n",
    "    x, y = np.pad(x, (0, max_len - len(x)), 'constant'), np.pad(y, (0, max_len - len(y)), 'constant')\n",
    "    return np.linalg.norm(x - y)\n",
    "    ### END YOUR CODE\n",
    "\n",
    "\n",
    "def dtw(obs1: list, obs2: list, dist_fn) -> float:\n",
    "    \"\"\"\n",
    "    Compute the dynamic time warping score between two observations.\n",
    "    \n",
    "    Arguments:\n",
    "    obs1: List of first observations.\n",
    "    obs2: List of second observations.\n",
    "    dist_fn: Similarity function to use.\n",
    "\n",
    "    Returns the score as float.\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    m, n = len(obs1), len(obs2)\n",
    "    D = np.full((m + 1, n + 1), np.inf, dtype = float)\n",
    "    D[0, 0] = 0\n",
    "\n",
    "    for i in range(1, m + 1):\n",
    "        for j in range(1, n + 1):\n",
    "            cost = dist_fn(obs1[i - 1], obs2[j - 1])\n",
    "            D[i, j] = cost + min(D[i - 1, j],\n",
    "                                D[i, j - 1],\n",
    "                                D[i - 1, j - 1])\n",
    "\n",
    "    return D[m, n]\n",
    "    ### END YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcf1446-f4ce-492f-bab6-2f9c21bb066c",
   "metadata": {},
   "source": [
    "### Experiment 1: DTW scores\n",
    "\n",
    "For each speaker and digit, select one recording as an observation (obs1) and the others as tests (obs2). How do scores change across speakers and across digits?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2661aed-5aaf-416e-8938-c2948d272e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE\n",
    "for speaker in speakers:\n",
    "    for digit in range(10):\n",
    "        filtered = [a for a in audios if a['digitLabel'] == digit and a['speakerName'] == speaker]\n",
    "        obs1 = filtered[0]\n",
    "\n",
    "        for i in range(1, len(filtered)):\n",
    "            print(\"DTW score {} for {} saying '{}' compared to the first sample.\".format(\n",
    "                dtw(obs1['mfccs'], filtered[i]['mfccs'], dist_fn = dist), speaker.capitalize(), digit\n",
    "            ))\n",
    "### END YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98395418-830b-4db6-8193-78c66246090a",
   "metadata": {},
   "source": [
    "### Implement a DTW-based Isolated Word Recognizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a985657-6e6e-42e7-b49b-6a33083ae575",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: Classify recording into digit label based on reference audio recordings\n",
    "\n",
    "def recognize(obs: List[Tuple[float]], refs: List[Audio]) -> str:\n",
    "    \"\"\"\n",
    "    Classify the input based on a reference list (train recordings).\n",
    "    \n",
    "    Arguments:\n",
    "    obs: List of input observations (MFCCs).\n",
    "    refs: List of audio items (train recordings).\n",
    "    \n",
    "    Returns classname where distance of observations is minumum.\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    min_distance = np.inf\n",
    "    best_label = None\n",
    "    \n",
    "    obs = np.array(obs)\n",
    "    \n",
    "    for ref in refs:      \n",
    "        distance = dtw(obs, np.array(ref['mfccs']), dist_fn = dist)\n",
    "        \n",
    "        if distance < min_distance:\n",
    "            min_distance = distance\n",
    "            best_label = ref['digitLabel']\n",
    "    \n",
    "    return str(best_label)\n",
    "    ### END YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef4f2c0-8cc2-4062-88ea-986619267471",
   "metadata": {},
   "source": [
    "### Experiment 2: Speaker-Dependent IWR\n",
    "\n",
    "Select training recordings from one speaker $S_i$ and disjoint test recordings from the same speaker $S_i$. Compute the Precision, Recall, and F1 metrics, and plot the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc09a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26296f18-6589-4750-ad84-84211f0f04ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE\n",
    "data = [a for a in audios if a['speakerName'] == 'george']\n",
    "\n",
    "np.random.shuffle(data)\n",
    "train_set, test_set = train_test_split(data)\n",
    "\n",
    "predictions = [recognize(np.array(test['mfccs']), train_set) for test in test_set]\n",
    "true_labels = [str(test['digitLabel']) for test in test_set]\n",
    "\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predictions, average='macro')\n",
    "\n",
    "print(f\"Precision: {precision}, Recall: {recall}, F1-Score: {f1}\")\n",
    "\n",
    "cm = confusion_matrix(true_labels, predictions)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.show()\n",
    "### END YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39bfaa8-1447-4e3a-b6b2-48ac96857780",
   "metadata": {},
   "source": [
    "### Experiment 3: Speaker-Independent IWR\n",
    "\n",
    "Select training recordings from one speaker $S_i$ and test recordings from another speaker $S_j$. Compute the Precision, Recall, and F1 metrics, and plot the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88904192-86a3-48fd-8050-a0f6522441d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE\n",
    "train_set = [a for a in audios if a['speakerName'] == 'george']\n",
    "test_set = [a for a in audios if a['speakerName'] == 'jackson']\n",
    "\n",
    "predictions = [recognize(np.array(test['mfccs']), train_set) for test in test_set]\n",
    "true_labels = [str(test['digitLabel']) for test in test_set]\n",
    "\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predictions, average='macro')\n",
    "\n",
    "print(f\"Precision: {precision}, Recall: {recall}, F1-Score: {f1}\")\n",
    "\n",
    "cm = confusion_matrix(true_labels, predictions) \n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.show()\n",
    "### END YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6be08a1-842b-45ad-b769-61a172333e96",
   "metadata": {},
   "source": [
    "### Food for Thought\n",
    "\n",
    "- What are inherent issues of this approach?\n",
    "- How does this algorithm scale with a larger vocabulary, how can it be improved?\n",
    "- How can you extend this idea to continuous speech, ie. ?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
