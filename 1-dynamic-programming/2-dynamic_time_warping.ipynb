{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc048d56-62ad-4b17-a523-b6201f9687f4",
   "metadata": {},
   "source": [
    "# Assignment 1: Dynamic Time Warping\n",
    "\n",
    "---\n",
    "\n",
    "## Task 4) Isolated Word Recognition\n",
    "\n",
    "Due to the relatively large sample number (e.g. 8kHz), performing [DTW](https://en.wikipedia.org/wiki/Dynamic_time_warping) on the raw audio signal is not advised (feel free to try!).\n",
    "A better solution is to compute a set of features; here we will emtract [mel-frequency cepstral coefficients](https://en.wikipedia.org/wiki/Mel-frequency_cepstrum) over windows of 25ms length, shifted by 10ms.\n",
    "Recommended implementation is [librosa](https://librosa.org/doc/main/generated/librosa.feature.mfcc.html).\n",
    "\n",
    "### Data\n",
    "\n",
    "Download Zohar Jackson's [free spoken digit dataset](https://github.com/Jakobovski/free-spoken-digit-dataset).\n",
    "There's no need to clone, feel free to use a revision, like [v1.0.10](https://github.com/Jakobovski/free-spoken-digit-dataset/archive/refs/tags/v1.0.10.tar.gz).\n",
    "File naming convention is trivial (`{digitLabel}_{speakerName}_{indem}.wav`); let's restrict to two speakers, eg. `jackson` and `george`.\n",
    "\n",
    "### Dynamic Time Warping\n",
    "\n",
    "[DTW](https://en.wikipedia.org/wiki/Dynamic_time_warping) is closely related to [Levenshtein distance](https://en.wikipedia.org/wiki/Levenshtein_distance) and [Needleman-Wunsch algorithm](https://en.wikipedia.org/wiki/Needlemanâ€“Wunsch_algorithm).\n",
    "The main rationale behind DTW is that the two sequences are can be aligned but their speed and exact realization may very.\n",
    "In consequence, cost is not dependent on an edit operation but on a difference in observations.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638e594f-2ef3-4d92-862c-4bb6a01ff21b",
   "metadata": {},
   "source": [
    "### Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9b8ab0-b4d8-483f-8eab-a0112bab5e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa as lr\n",
    "import os\n",
    "from typing import List, Tuple, TypedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ffdd0b2f-5e6c-4fb4-95b0-217e75aa5b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: Read in files, compute MFCC, and organize\n",
    "### Notice: You can restrict the number to a few files for each speaker-digit\n",
    "\n",
    "class Audio(TypedDict):\n",
    "    digitLabel: int\n",
    "    speakerName: str\n",
    "    index: int\n",
    "    mfccs: List[Tuple[float]]\n",
    "\n",
    "audios: List[Audio] = []\n",
    "\n",
    "speakers = [\"george\", \"jackson\", \"yweweler\"]\n",
    "\n",
    "### YOUR CODE HERE\n",
    "FOLDER = 'data/free-spoken-digit-dataset-1.0.10/recordings'\n",
    "\n",
    "for file in os.listdir(FOLDER):\n",
    "    if not file.endswith('.wav'):\n",
    "        continue\n",
    "\n",
    "    parts = file.split('_')\n",
    "    digit, name, index = int(parts[0]), parts[1], int(parts[2].split('.')[0])\n",
    "\n",
    "    # Limit the number of speakers and spoken digits\n",
    "    if name not in speakers or index > 9:\n",
    "        continue\n",
    "\n",
    "    y, sr = lr.load(FOLDER+os.sep+file)\n",
    "\n",
    "    mfccs = lr.feature.mfcc(y=y, sr=sr, n_mfcc=10, hop_length=int(0.010 * sr), win_length=int(0.025 * sr))\n",
    "    mean_mfccs = np.mean(mfccs, axis=1)\n",
    "    \n",
    "    audios.append(Audio(\n",
    "        digitLabel = digit,\n",
    "        speakerName = name,\n",
    "        index = index,\n",
    "        mfccs = mean_mfccs\n",
    "    ))\n",
    "### END YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125e920b-3296-44aa-87b9-81d73f515cc3",
   "metadata": {},
   "source": [
    "### Implement Dynamic Time Warping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4a7476e6-be89-41c5-81f4-dd6ab331a748",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist(x: Tuple[float], y: Tuple[float]) -> float:\n",
    "    \"\"\"\n",
    "    Compute the distance between two samples.\n",
    "\n",
    "    Arguments:\n",
    "    x: MFCCs of first sample.\n",
    "    y: MFCCs of second sample.\n",
    "\n",
    "    Returns the distance as float\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    return np.linalg.norm(x - y)\n",
    "    ### END YOUR CODE\n",
    "\n",
    "\n",
    "def dtw(obs1: list, obs2: list, dist_fn) -> float:\n",
    "    \"\"\"\n",
    "    Compute the dynamic time warping score between two observations.\n",
    "    \n",
    "    Arguments:\n",
    "    obs1: List of first observations.\n",
    "    obs2: List of second observations.\n",
    "    dist_fn: Similarity function to use.\n",
    "\n",
    "    Returns the score as float.\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    m, n = len(obs1), len(obs2)\n",
    "    D = np.full((m + 1, n + 1), np.inf, dtype = float)\n",
    "    D[0, 0] = 0\n",
    "\n",
    "    for i in range(1, m + 1):\n",
    "        for j in range(1, n + 1):\n",
    "            cost = dist_fn(obs1[i - 1], obs2[j - 1])\n",
    "            D[i, j] = cost + min(D[i - 1, j],\n",
    "                                D[i, j - 1],\n",
    "                                D[i - 1, j - 1])\n",
    "            \n",
    "    return D[m, n]\n",
    "    ### END YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcf1446-f4ce-492f-bab6-2f9c21bb066c",
   "metadata": {},
   "source": [
    "### Experiment 1: DTW scores\n",
    "\n",
    "For each speaker and digit, select one recording as an observation (obs1) and the others as tests (obs2). How do scores change across speakers and across digits?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f2661aed-5aaf-416e-8938-c2948d272e80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric for 'George' on '0': max 170.54673719406128, min 120.58198595046997, avg 149.59494029151068\n",
      "Metric for 'George' on '1': max 171.94083440303802, min 88.18925929069519, avg 130.76963586939706\n",
      "Metric for 'George' on '2': max 219.62000608444214, min 69.1938362121582, avg 102.41382263766394\n",
      "Metric for 'George' on '3': max 106.50680422782898, min 30.00497317314148, avg 56.75438544485304\n",
      "Metric for 'George' on '4': max 88.69950485229492, min 50.65685510635376, avg 66.24619430965848\n",
      "Metric for 'George' on '5': max 92.25510096549988, min 26.332313537597656, avg 53.193426847457886\n",
      "Metric for 'George' on '6': max 56.33814835548401, min 29.681915998458862, avg 41.97587177488539\n",
      "Metric for 'George' on '7': max 52.94770050048828, min 20.90135669708252, avg 37.10005389319526\n",
      "Metric for 'George' on '8': max 188.43460369110107, min 38.93387317657471, avg 73.99149809943304\n",
      "Metric for 'George' on '9': max 201.98564624786377, min 55.223432540893555, avg 83.96926190124617\n",
      "Metric for 'Jackson' on '0': max 113.73457193374634, min 47.464274883270264, avg 87.67766384945975\n",
      "Metric for 'Jackson' on '1': max 110.71123194694519, min 47.28130757808685, avg 70.77094070778952\n",
      "Metric for 'Jackson' on '2': max 227.16582536697388, min 64.35487365722656, avg 171.30276346206665\n",
      "Metric for 'Jackson' on '3': max 129.42563438415527, min 53.89637470245361, avg 87.2547370062934\n",
      "Metric for 'Jackson' on '4': max 224.74833726882935, min 57.30258226394653, avg 150.86384269926282\n",
      "Metric for 'Jackson' on '5': max 155.11084985733032, min 29.036773681640625, avg 88.54017771614923\n",
      "Metric for 'Jackson' on '6': max 75.8694896697998, min 24.319090366363525, avg 54.925201733907066\n",
      "Metric for 'Jackson' on '7': max 97.9636607170105, min 20.282804489135742, avg 71.65822913911607\n",
      "Metric for 'Jackson' on '8': max 134.2320737838745, min 48.74389410018921, avg 76.68310274018182\n",
      "Metric for 'Jackson' on '9': max 140.14691925048828, min 63.840471267700195, avg 93.34949254989624\n",
      "Metric for 'Yweweler' on '0': max 121.85890674591064, min 41.21236515045166, avg 81.47219671143426\n",
      "Metric for 'Yweweler' on '1': max 228.58583974838257, min 118.62406015396118, avg 165.9066330856747\n",
      "Metric for 'Yweweler' on '2': max 125.0506432056427, min 47.83172273635864, avg 80.22000720103581\n",
      "Metric for 'Yweweler' on '3': max 158.71823406219482, min 86.48726272583008, avg 126.7154122988383\n",
      "Metric for 'Yweweler' on '4': max 152.99374318122864, min 56.59793400764465, avg 98.30872493320041\n",
      "Metric for 'Yweweler' on '5': max 136.12835884094238, min 55.470478534698486, avg 87.2672463523017\n",
      "Metric for 'Yweweler' on '6': max 168.96879124641418, min 60.18233060836792, avg 110.58381635612912\n",
      "Metric for 'Yweweler' on '7': max 202.0361042022705, min 56.11700761318207, avg 125.2387757036421\n",
      "Metric for 'Yweweler' on '8': max 116.69997954368591, min 42.93594229221344, avg 75.57518589496613\n",
      "Metric for 'Yweweler' on '9': max 175.54160594940186, min 47.662238121032715, avg 80.13517469829983\n"
     ]
    }
   ],
   "source": [
    "### YOUR CODE HERE\n",
    "for speaker in speakers:\n",
    "    for digit in range(10):\n",
    "        filtered = [a for a in audios if a['digitLabel'] == digit and a['speakerName'] == speaker]\n",
    "        obs1 = filtered[0]\n",
    "\n",
    "        scores = []\n",
    "        for i in range(1, len(filtered)):\n",
    "            scores.append(dtw(obs1['mfccs'], filtered[i]['mfccs'], dist_fn=dist))\n",
    "\n",
    "        print(\"Metric for '{}' on '{}': max {}, min {}, avg {}\".format(\n",
    "            speaker.capitalize(), digit, max(scores), min(scores), np.average(scores)\n",
    "        ))\n",
    "### END YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98395418-830b-4db6-8193-78c66246090a",
   "metadata": {},
   "source": [
    "### Implement a DTW-based Isolated Word Recognizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a985657-6e6e-42e7-b49b-6a33083ae575",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: Classify recording into digit label based on reference audio recordings\n",
    "\n",
    "def recognize(obs: List[Tuple[float]], refs: List[Audio]) -> str:\n",
    "    \"\"\"\n",
    "    Classify the input based on a reference list (train recordings).\n",
    "    \n",
    "    Arguments:\n",
    "    obs: List of input observations (MFCCs).\n",
    "    refs: List of audio items (train recordings).\n",
    "    \n",
    "    Returns classname where distance of observations is minumum.\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    min_distance = np.inf\n",
    "    best_label = None\n",
    "    \n",
    "    obs = np.array(obs)\n",
    "    \n",
    "    for ref in refs:      \n",
    "        distance = dtw(obs, np.array(ref['mfccs']), dist_fn = dist)\n",
    "        \n",
    "        if distance < min_distance:\n",
    "            min_distance = distance\n",
    "            best_label = ref['digitLabel']\n",
    "    \n",
    "    return str(best_label)\n",
    "    ### END YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef4f2c0-8cc2-4062-88ea-986619267471",
   "metadata": {},
   "source": [
    "### Experiment 2: Speaker-Dependent IWR\n",
    "\n",
    "Select training recordings from one speaker $S_i$ and disjoint test recordings from the same speaker $S_i$. Compute the Precision, Recall, and F1 metrics, and plot the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc09a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26296f18-6589-4750-ad84-84211f0f04ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE\n",
    "data = [a for a in audios if a['speakerName'] == 'george']\n",
    "\n",
    "np.random.shuffle(data)\n",
    "train_set, test_set = train_test_split(data)\n",
    "\n",
    "predictions = [recognize(np.array(test['mfccs']), train_set) for test in test_set]\n",
    "true_labels = [str(test['digitLabel']) for test in test_set]\n",
    "\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predictions, average='macro')\n",
    "\n",
    "print(f\"Precision: {precision}, Recall: {recall}, F1-Score: {f1}\")\n",
    "\n",
    "cm = confusion_matrix(true_labels, predictions)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.show()\n",
    "### END YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39bfaa8-1447-4e3a-b6b2-48ac96857780",
   "metadata": {},
   "source": [
    "### Experiment 3: Speaker-Independent IWR\n",
    "\n",
    "Select training recordings from one speaker $S_i$ and test recordings from another speaker $S_j$. Compute the Precision, Recall, and F1 metrics, and plot the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88904192-86a3-48fd-8050-a0f6522441d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE\n",
    "train_set = [a for a in audios if a['speakerName'] == 'george']\n",
    "test_set = [a for a in audios if a['speakerName'] == 'jackson']\n",
    "\n",
    "predictions = [recognize(np.array(test['mfccs']), train_set) for test in test_set]\n",
    "true_labels = [str(test['digitLabel']) for test in test_set]\n",
    "\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predictions, average='macro')\n",
    "\n",
    "print(f\"Precision: {precision}, Recall: {recall}, F1-Score: {f1}\")\n",
    "\n",
    "cm = confusion_matrix(true_labels, predictions) \n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.show()\n",
    "### END YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6be08a1-842b-45ad-b769-61a172333e96",
   "metadata": {},
   "source": [
    "### Food for Thought\n",
    "\n",
    "- What are inherent issues of this approach?\n",
    "- How does this algorithm scale with a larger vocabulary, how can it be improved?\n",
    "- How can you extend this idea to continuous speech, ie. ?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
